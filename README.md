<h1 align="center">üéß Involves</h3>
<h2 align="center">üé§ Aline da Silveira Rosa</h1>
<h3 align="center">üé∂ Teste T√©cnico: Pessoa Engenheira de Dados</h3>

<hr>

<h3>1)	Descreva com suas palavras os principais conceitos abaixo:</h2>

<h4>a) O que √© um Data Warehouse?</h4>

<p align="justify">
Um Data Warehouse, ou armaz√©m de dados, como tamb√©m √© chamado, √© uma estrutura de banco de dados relacionais utilizada para an√°lise de dados em formato estruturado, utilizando dos conceitos de OLAP para a segrega√ß√£o de dados em silos de informa√ß√£o, onde cada data Warehouse armazena dados de determinado setor, projeto ou cliente. Em um exemplo mais claro, em uma empresa que conta com os setores ‚ÄúRecursos Humanos‚Äù, ‚ÄúMarketing‚Äù e ‚ÄúVendas‚Äù, cada setor tem um Data Warehouse com dados pertinentes relativos ao seu dia a dia para que analistas possam gerar conhecimento e informa√ß√£o por meio das an√°lises efetuadas. Dessa forma, os mesmos dados podem ser analisados por diferentes perspectivas, enquanto o setor ‚ÄúVendas‚Äù pode analisar valores totais de vendas, o setor ‚ÄúMarketing‚Äù pode analisar o alcance de uma campanha realizada dentro do per√≠odo, enquanto ‚ÄúRecursos Humanos‚Äù pode analisar a efici√™ncia de determinado vendedor durante a campanha realizada.</p>

<h4>b) Quais caracter√≠sticas possuem as tabelas do tipo Fato e Dimens√£o?</h4>

<p align="justify">
Os conceitos de tabelas dimens√£o e fato v√™m da utiliza√ß√£o da modelagem de dados em formato de estrela, amplamente utilizado por data warehouses relacionais, onde tabelas fato armazenam eventos, tais como uma venda, uma compra, uma contrata√ß√£o ou demiss√£o, enquanto as tabelas dimens√£o armazenam as descri√ß√µes (nome, endere√ßo, descri√ß√£o, categorias etc) das entidades. Tais tabelas se relacionam por chaves de identifica√ß√£o √∫nicas. Uma tabela dimens√£o precisa, necessariamente, conter um registro √∫nico para cada entrada, enquanto uma tabela fato pode conter mais de uma entrada por identificador. Por exemplo, em uma tabela fato, um cliente (id_cliente) pode realizar mais de uma compra (id_compra), mas em uma tabela dimens√£o, uma compra (id_compra) s√≥ pode ser efetuada por um cliente (id_cliente).</p>

<h4>c) O que √© ETL?</h4>
<p align="justify">
De forma bastante resumida, ETL √© o processo de Extra√ß√£o, Transforma√ß√£o e Carga de dados, onde extra√ß√£o se refere √† coleta dos dados na fonte, podendo ser um banco de dados transacional (OLTP), arquivos como json, cvs, m√≠dias ou demais arquivos n√£o relacionais; transforma√ß√£o se refere ao tratamento necess√°rio para que os dados sejam normalizados, como limpeza de campos num√©ricos, padroniza√ß√£o, altera√ß√£o de tipagem de dados; e carga se refere √† disponibiliza√ß√£o dos dados para an√°lise, podendo ser a grava√ß√£o dos dados em um data warehouse, a entrega de um cubo de dados modelado para aplica√ß√£o de an√°lises, a disponibiliza√ß√£o de arquivos csv ou demais necessidades pertinentes ao cliente.</p>

<h4>d) Quais s√£o as principais atribui√ß√µes de um Engenheiro de Dados?</h4>
<p align="justify">
As principais atribui√ß√µes de um Engenheiro de Dados est√£o em construir e sustentar pipelines de extra√ß√£o de dados (ETL) automatizados e confi√°veis que entreguem aos analistas e cientistas de dados os dados necess√°rios para utiliza√ß√£o no desenvolvimento de suas atividades, nos diversos formatos poss√≠veis, com o devido cuidado para que as extra√ß√µes n√£o causem impactos negativos nos sistemas transacionais de forma a prejudicar a atividade de produ√ß√£o.</p>

<h4>e) O que √© Trade Marketing?</h4>
<p align="justify">
Trade Marketing √© a estrat√©gia de marketing que utiliza uma s√©rie de metodologias e t√©cnicas para potencializar a comercializa√ß√£o de produtos em pontos de vendas, pensando na rela√ß√£o entre produtor (empresa, marca), ponto de venda (presencial ou digital), e consumidor (direto ou indireto), indo desde an√°lise de estoque e efici√™ncia de reposi√ß√£o de mercadorias at√© o estudo do comportamento do consumidor final.</p>

<hr>

<h3 align="center">Quest√µes 08, 09 e 10</h3>

<h4>Para as quest√µes envolvendo o Pentaho, seguem as justificativas:</h4>

<p>Primeiramente, criei uma inst√¢ncia do MySQL na nuvem Azure, como forma de poder testar corretamente as transforma√ß√µes e conex√µes solicitadas.</p>
	<p>Dentro do Pentaho, utilizei uma transforma√ß√£o que l√™ um arquivo json onde est√£o salvos os par√¢metros utilizados para a conex√£o com o banco de dados, dessa forma os dados de conex√£o n√£o ficam expostos dentro dos arquivos ktr. Por padr√£o, adiciono a extens√£o .json ao .gitignore, com isso posso salvar usu√°rio e senha dentro do arquivo e ele n√£o ser√° disponibilizado junto com os commits. Utilizo tamb√©m um step de envio de e-mail notificando o administrador do pipeline quando h√° erro em algum dos steps, para isso os valores de login do e-mail est√£o adicionados no mesmo arquivo de par√¢metros.</p>
	<p>Utilizei o ‚Äúdummy‚Äù para preenchimento dos par√¢metros de conex√£o automaticamente quando estou realizando testes nas transforma√ß√µes e, para n√£o precisar subir altera√ß√µes nas branches todas as vezes em que modifico o hop, deixo na transforma√ß√£o por padr√£o, facilitando testes.</p>
	<p>Em seguida, utilizei uma transforma√ß√£o para inserir no banco de dados algumas informa√ß√µes de log com o hor√°rio de in√≠cio do job (hor√°rio do sistema), ID de execu√ß√£o (campo incremental) e nome do job (preenchido manualmente no data grid).</p>
	<p>A seguir, separei o que foi solicitado em quatro jobs:</p>
<p>a)	Extraction: neste job, separo as fontes em pastas como forma de organiza√ß√£o dos arquivos, mesmo sendo apenas um csv quis deixar organizado pra manter um padr√£o. Aqui, fa√ßo a leitura do csv e insiro os dados em uma tabela tempor√°ria, que ser√° truncada em todas as execu√ß√µes para realizar uma carga completa dos arquivos no diret√≥rio.</p>
<p>b)	Merge: usando os dados da tabela tempor√°ria (stg.temp_data_import), comparo o ID fornecido com a tabela que armazenar√° todos os dados em stage, pensando nos conceitos de data lake, para n√£o perder informa√ß√µes com as transforma√ß√µes que ser√£o realizadas. Os dados j√° existentes na tabela stg.data_import ser√£o atualizados, e os novos dados ser√£o inseridos.</p>
<p>c)	Transformation: a partir daqui, utilizei como fonte a tabela stg.data_import, onde est√£o inseridos os dados sem tratamento, para a constru√ß√£o das tabelas dimens√£o e fato. N√£o costumo utilizar as ferramentas datawarehouse fornecida pelo pentaho por costume de trabalhar com altos volumes de dados e j√° haver reparado que existe uma queda de performance, ent√£o utilizei os steps de script SQL para realizar as transforma√ß√µes necess√°rias. Caso fossem tabelas maiores, o indicado seria salvar os aquivos .sql em um diret√≥rio e realizar a leitura dos mesmos via pentaho, para que a execu√ß√£o n√£o seja por dentro do programa e sim em background, o que gera aumento de performance significativo. Entretanto, encontrei alguns erros de sintaxe com o MySQL (estou mais habituada ao SQL Server no dia a dia) e, como estava com pouco tempo h√°bil para a execu√ß√£o do pipeline, resolvi manter a leitura via pentaho.</p>
<p>d)	Load: esta etapa n√£o foi solicitada, mas pensando no padr√£o de integra√ß√£o e produto de dados, achei interessante criar um exemplo de dataset com a carga final utilizando as tabelas dimens√£o e fato criadas anteriormente.</p>
<p>Por √∫ltimo, utilizei uma transforma√ß√£o para atualizar a tabela de logs, inserindo novamente o hor√°rio do sistema, agora no campo end_time, gerando assim um controle tanto do hor√°rio em que a extra√ß√£o come√ßou, quanto do momento em que a carga foi finalizada.</p>
